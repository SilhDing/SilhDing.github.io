## 3. Spark: Cluster Computing with Working Sets

**Spark**: supports these applications while retaining the scalability and fault tolerance of MapReduce.

- introduces an abstraction called resilient distributed datasets (RDDs)
- MapReduce/Dryad: providing a programming model where the user creates acyclic data ﬂow graphs to pass input data through a set of operators; **but cannot handle some kinds of aplications**:
    - Iterative job: many ML algorithm
    - Iterative analytics
- RDD: represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost.
- **Spark**: the ﬁrst system to allow an efﬁcient, general-purpose programming language to be used interactively to process large datasets on a cluster.

### Programming model

Spark provides two main abstractions for parallel programming: **resilient distributed datasets** and **parallel operations on these datasets**

#### Resilient distributed datasets

- elements of an RDD need not exist in physical storage
- a handle to an RDD contains enough information to compute the RDD starting from data in reliable storage
- each is represented by a Scala object: construct RDDs in 4 ways

#### Parallel Operations

- **reduce**: combines dataset elements using an associative function to produce a result at the driver program.
- **collect**: Sends all elements of the dataset to the driver program.
- **foreach**: Passes each element through a user provided function.

#### Shared Variables

Spark also lets programmers create two restricted types of shared variables to support two simple but common usage patterns:
- **Broadcast variables**: create a “broadcast variable” object that wraps the value and ensures that it is only copied to each worker once.
- **Accumulators**: workers can only “add” to using an associative operation, and that only the driver can read.

### Example

-

### Implementation

- Spark is built on top of Mesos (WOW)...
- core of Spark is the implementation of resilient distributed datasets
- (**Impoertant**) Each dataset object contains a pointer to its parent and information about how the parent was transformed
- each RDD object implements the same simple interface: **getPartitions**, **getIterator**, **getPreferredLocations**
- The different types of RDDs differ only in how they implement the RDD interface (**see examples on paper**)
- **Shared variables** are implemented using classes with custom serialization formats
- **Interpreter Integration**: integrated Spark into the Scala interpreter (refer to paper)

### Results

**Much better than Hadoop**



