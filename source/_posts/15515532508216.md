# Prefetching: Sibling of Caching

- crucial for streaming data over Internet;
- rising star as bandwidth improves, but latency stagnates

## Prefetching is about spatial locality

- Caching was about temporal locality
    - “Future will reflect the past”
    - often correct, sometimes wrong
- Prefetching makes a different bet
    - “Future may differ from the past, but I can predict it”
    - often wrong, sometimes correct

## Big Pictures

- "Prefetching": initiating fetch of data **in advance of need**
- **Completion** of prefetch may occur before or after need occurs
    - before -> complete masking of access latency
    - after -> reduced effective latency
- Sounds great, but?
    - commitment of critical resources based on imperfect knowledge
    - erroneous commitment may increase latency
        - busy server, locks held, clogged network, full local cache, ...
    - demand request delayed behind prefetching traffic
- Perils

![-w982](media/15515532508216/15515538072969.jpg)

- **Prefetching most valuable when high latency & high bw!**
    - High latency and low BW? Life it hard, just give up.
    - Low latency and low BW? Alas, knowledge alone is not enough (case 3)
    - Low latency and high BW? Life is wonderful. Why bother prefetching?
- With high latency and high BW
    - early knowledge + resources to act on that knowledge
    - prefetching safer with higher bandwidth
    - prefetching more valuable with high latency

## Why important?

- **Caching and prefetching only known techniques for reducing latency**!
- prefetching is the only way to eliminate cold misses

## Knowledge for Prefetching

Sources of prefetching hints:
- reference stream (e.g. sequential access)
- explicit user hints (Coda hoarding)
- explicit programmer hints
- compiler extraction of hints from source code
- historical observation (previous executions)
    - important special case is branch prediction in hardware
    - another important special case is value prediction
- higher-level context awareness
    - online calendars, email analysis, crowd behavior analysis

## Timing is everything!

**If cache space is plentiful**
- no replacements
- cold cache filling is all that matters
- no harm in prefetching as early as possible
- only resource that is scarce is network bandwidth (and server)
- only harm: queueing delay for demand misses, bandwidth wasted

**If cache space is scarce**
- replacements are frequent
- prefetch too early -> evict before first use!

**Prefetch too late**
- data is not yet in cache when demand access arrives
- may reduce (but not eliminate) cold cache miss delay

## Speculation

Prefetching is a form of **data speculation**!

Data staging is another example of data speculation
- prefetch to a nearby cache rather than to local cache
- you may not have connectivity to that cache but will “soon”
    - e.g. cache at arrival gate when you step off the plane

Complement of data speculation is **speculative execution**
- hot topic in OS research
- especially interesting in a multi-core context
    - many cores available for “waste” in order to make speculative bets