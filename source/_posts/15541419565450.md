# Design Protocol of Auto-scaling Web Server
by Yihang Ding (Andrew ID: yihangd)

## Types of Servers

In this system, there are three main types of servers.

1. Master server. There is always **only one** master server in the system. The master server is responsible for keeping a central queue for all incoming requests, recording all other servers' information, initializing the cache, and starting or closing servers (scale-in or scale-out). Please note that the master server does not detect the condition where scaling for middle-tier is needed, but it would check the current number of middle-end servers and adjust the number of front-end servers correspondingly. The master server would also get requests from load balancer, **acting as a front-end server**.
2. Front-end server. There may be multiple front-end servers in the system. Each front-end server simply receives requests and pushes it into the queue on master server via RMI. **It is not involved in any activities with scale-in or scale-out**.
3. Middle-end server. There may be multiple middle-end servers in the system. Each middle-end server would get a request from master server and process it with the cache. In addition, it would also detect whether scale-in or scale-out for middle-tier is necessary, and, if yes, it would request the master server to add or close  middle-end servers correspondingly.

## Initial Server Number

Once started, the system would start two servers: one master server (also a front-end server) and one middle-end server. Then, it would measure the rate of the request through the very first several requests and determine the best number of front-end and middle-end servers. For example, if the requests interval falls into the range `[200 msï¼Œ 300 ms]`, then using 2 front-end servers and 4 middle-end servers is optimal. The optical number is determined by **benchmarking and experiments** with different loads (please note the master server is included in the table):


| Interval (ms) | Front-end | Middle-end |
| --- | --- | --- |
| `[0, 100]` | 3 | 8 |
| `[100, 200]` | 2 | 6 |
| `[200, 300]` | 2 | 4 |
| `[300, 500]` | 1 | 3 |
| `[500, Inf]` | 1 | 2 |


## Add or Remove Server (Scale Policy)

### Middle Tier

Scaling on middle tier is **determined by the middle-end servers** and **done by master server**.

**Scale out**: when one middle-end server gets a request from master server, it would also check if the queue on master server is too long. If so (the queue length is larger than the number of middle-end servers), we call "a too long queue is detected". If the server **detect "a too long queue" for consecutive two times**, we start two more middle-end servers.

**Scale in**: the central queue on master server is implemented by `BlockingQueue`, thus, if there is no request in the queue, the middle-end would wait for a while, and then get `null` if it has waited for too long (timeout). If the middle-end server **get three consecutive `null` requests** (three consecutive timeout), it would notify the master to scale in, i.e., shut down the current middle-end server.

### Front Tier

**The number of front-end servers is dynamically adjusted by number of the middle-end servers**, which is done by the master server. The master server would iteratively check the number of middle-end servers and determine if scaling on front tier is needed. Under such a policy, we could always keep an optimal ratio of front-end and middle-end server (the optimal ratio is discovered by benchmarking and experiments, as shown in the table on the last page).

## Cache

The cache would store all `get` requests in a `HashMap`. Thus, the cache would return some results without connecting to the database tier if the query is stored in the cache. The cache also passes through any misses as well as `set` and `transaction` operations. Invalidation or update of cache entries is not considered in this system. The cache is initialized on the master server and connected with middle-end server via RMI.

## Conclusions

Through this project, some observation listed below may be helpful in designing an auto-scaling system:

- Adding more tiers would be helpful to achieve high-performance scaling. For example, if all servers are responsible for both receiving and processing requests, it is highly possible that some requests would not be accepted before timeout. Thus, dividing the servers into more than one tier and letting them focus on "smaller" tasks is more efficient;

- Benchmarking is highly important in making the scaling policy. They would produce some constructive data for you in implementing the scaling policy. In fact, the arrival of requests is random and unpredictable (or only predictable at a coarse granularity), so heuristic experiments or benchmarking would help us decide the size of the each tier, as there is no panacea for solving such a problem. 