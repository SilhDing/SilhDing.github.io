## 4. MapReduce: Simpliﬁed Data Processing on Large Clusters

Users specify a **map** function that processes a key/value pair to generate a set of intermediate key/value pairs, and a **reduce** function that merges all intermediate values associated with the same intermediate key. 

### Programming Model

- takes a set of input key/value pairs
- produce a set of output key/value pairs
- **Map**: takes an input pair and produces a set of intermediate key/value pairs
- **Reduce**: accepts an intermediate key I and a set of values for that key




### Implementation

- Environment: large clusters of commodity PCs connected together with switched Ethernet

#### Execution overview

Please see the figure.

![-w783](media/15492261602503/15492581204164.jpg)

Sequence of actions:
1. splits the input file into pieces (16 - 64 MB); starts up many copies of the program on a cluster of machines
2. One of the copies of the program is special – the master; the rest are workers assigned by the master (M map tasks and R reduce tasks)
3. map task: parses key/value pairs out of the input data and passes each pair to the user-deﬁned Map function. The intermediate key/value pairs produced by the Map function are buffered in memory.
4. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers.
5. When Reduce knows the locations -> use RPC to read the buffered data from the local disks of map workers; then group the results of same keys
6. passes the key and the corresponding set of intermediate values to the user’s Reduce function; output of the reduce function is appended to a final output file (R output files)

#### Master Data Structure

- For each map task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks).
- for each completed map task, the master stores the locations and sizes of the R intermediate file regions produced by the map task; updates to this location and size information are received as map tasks are completed

#### Fault tolerance

##### Worker Failure

- master pings every worker periodically; no response -> failed
- any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling
- Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible; Completed reduce tasks do not need to do this (results are stored on a global file system)
- A failed; the B does it -> all workers executing reduce tasks are notiﬁed of the reexecution

##### Master failure

- a new copy can be started from the last checkpointed state;
- unlikely to have failure; stop MapReduce; retry the function

#### Locality

- conserve network bandwidth by taking advantage of the fact that the **input data is stored on the local disks of the machines that make up our cluster**
- master takes the location information of the input files into account and attempts to schedule a map task on a machine that contains a replica of the corresponding input data; to schedule a map task near a replica of that task’s input data

#### Task Granularity

- practical bounds on how large M and R can be in our implementation, since the master must make O(M + R) scheduling decisions and keeps O(M ∗ R) state in memory

#### Backup tasks
- long time to finish: "straggler":a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation
- Why:
    - machine has bad disks
    - other tasks on that machine
    - processor cache disabled
- Solve: When a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks

### Refinement

### Performance
