# Caching: Making Far Look Near

The **only** technique to reduce end-to-end latency.

## Recap

- Expected cost of a reference = (Miss Ratio * cost of miss) + (Hit Ratio * cost of hit)
- Cache Advantage = (Cost of Miss / Cost of Hit)

## Cache is a double win

- local benefits
    - local copy -> lowest possible latency
        - No networking queuing
        - No server queuing or computation delay
        - No transmission delay
- Rest of the distributed system wins too!
    - resource utilization (“efficiency”)
    - response time (“agility”)
    - arrival pattern (“freedom”)

## Key components
- Fetch policy
- Update propagation
- Replacement policy

How to solve cold cache misses? Prefetching.

## Fetch policy

What data should you cache and when?

### What to fetch?

- Approach 1: full replication
    - every participating machine gets a full and complete copy
    - every new file gets transmitted to all replicas
    - every updated file gets propagated
    - All data is fetched **in advance to point of use**
    - Cons:
        - storage consumed on each replica;
        - update traffic
        - Machines receive updates whether they care or not (**push model**)
    - **it is in fact not caching** lol
- Approach 2: on-demand caching
    - requires integration with the operating system
    - fine-grained and selective approach to data management
    - total application transparency

### Update propagation

a.k.a., cache consistency

#### One-Copy Semantics

A caching (or replication) system has one-copy semantics iff: **there are no externally observable functional differences with respect to an equivalent system that does no caching (or replication)**

But why this is hard?
1. Physical master copy may not exist
    - hosts track who has most recent copy (like P2P)
2. Network may break between some users and master copy
    - disconnected sites see no further updates
    - updates by disconnected sites are not visible to others anymore
3. Intense read- and write-sharing across sites
    - huge amount of cache propagation traffic
    - cache would become more useless 

#### Consistency models

- **Broadcast Invalidations**
    - Every potential caching site notified on every update
    - Strictly "one-copy" and simple to implement!
    - Cons:
        - wasted traffic if no readers elsewhere
        - updating process blocked until invalidation complete
        - not a scalable design
- **check on use**
    - reader checks master copy before each use (conditional fetch)
    - done at coarse granularity 
    - at whole-file granularity
        - open {read | write}* close "session"
        - approximation to strict one-copy semantics at bit-granularity
    - Pros:
        - strict consistency at coarse granularity
        - easy to implement, no server state
        - servers don’t need to know of caching sites
    - Cons:
        - slows read access on loaded servers & high-latency networks
        - check is almost always success -> frivolous traffic
        - load on network and server
    - In fact, "Up to date" is relative to network latency

![-w753](media/15515415350444/15515466007092.jpg)

- **Callback**
    - targeted notification of caching sites
    - master copy tracks sites with cached data
    - typically done at coarse granularity (e.g. entire file)
    - on update, all sites with cached data notified
    - Pros:
        - excellent scalability for Unix workloads
        - zero network traffic for read of cached-valid objects
    - Cons:
        - sizable state on server
        - complexity of tracking cached state on clients
        - silence ambiguous for client
            - network failure -> lost callbacks 
            - periodic “keepalive” probes 
            - data could be stale between probes
- **Leases**
    - Caching site obtains finite-duration control from master copy
        - multiple sites can obtain read lease; only one can get write lease
    - lease duration = 0 -> Check on Use 
    - lease duration = infinite -> callback (Targeted Notification)
    - Pros:
        - generalizes the check on use and callback schemes
        - lease duration can be tuned to adapt to mutation rate
        - conceptually simple yet flexible
    - Cons:
        - writers delayed while read lease holders complete their leases
        - more traffic than callback (less than check-on-use)
- **Skip Scary Parts**
    - When write-sharing detected, turn off caching everywhere
    - Pros:
        - Precise single-copy semantics
        - Excellent fallback position
        - Good adaptation of caching aggressiveness to workload
    - Cons:
        - Server maintains state
        - Server aware of every use of data
- **Faith-Based Caching**
    - Or say, eventual consistency
    - blindly assume cached data is valid for a while
    - periodically check (based on time since last check)
    - no communication needed during trust period
    - small variant is a **TTL field** for each object (such as in web cache)
    - **Imprecise and weak approximation** to one-copy semantics (the above 5 methods used controlled and precise approximations)
    - Pros:
        - simple implementation
        - server is stateless
    - Cons:
        - User-visible inconsistencies sometimes seen (make)
        - Blind faith sometimes misplaced!
        - Not as efficient as callback-based schemes
- **Pass the buck**
    - Let the user trigger cache revalidation (hit “reload”)
    - otherwise, all cached copies assumed valid forever
    - Equivalent to infinite-TTL faith-based caching
    - **Arose in the context of the web**
    - Pros:
        - trivial to implement, no server changes
        - avoids frivolous cache maintenance traffic
    - Cons:
        - burdens on users
        - assumes existence of user: pain for write scripts/programs

### Cache replacement policy

#### **How to pick a victim?**        

- It involves in many factors
    - how soon evicted object might be needed again
    - how expensive it might be to fetch again
    - how much space is freed up by eviction
    - ...
    - **ideal one**: very large object that is not needed for a long time, and is very cheap to fetch again
- Vast majority of research assumes fixed size, fixed fetch cost, equal importance, thus the only significant variable is the **distance in the future to next reference to an object**
- But, how to predict distance to next reference?
    - If is is known, then it is simple: OPT is the (theoretical) optimal replacement algorithm: **pick object that will be referenced furthest in future**
    - LRU is often a good approximation to OPT: assumes recent past is a good predictor of the near future

#### Working Set

- **Given a time interval T, WorkingSet(T) is the set of distinct data objects accessed in T**
- For any given interval, T,
    - small working set -> small cache is enough  high locality
    - large working set -> poor locality
    - size and pages in working set may change over time

#### Is LRU always effective?

When does LRU work much worse than OPT?
- purely sequential access (scan)
    - No locality at all! cache does not help
- purely random access
    - being smart is useless; ratio of cache size to total data size is all that matters

    
Real multi-process workloads can be mixtures of individual behavior!
- some processes/threads may show locality
- others may show random or sequential behavior

